{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from six.moves import cPickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import nnet as nn\n",
    "import criteria as er\n",
    "import util\n",
    "import VAE\n",
    "import SVAE\n",
    "import update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_acc(pred, true):\n",
    "    ll = pred - true\n",
    "    ll = np.array(ll)\n",
    "    acc = 1 - (np.nonzero(ll)[0].shape[0])/float(ll.shape[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_file = 'timit_train_subset_1ok.npy'\\nvalid_file = 'timit_valid_1ok.npy'\\n#test_file = 'test_1ok.npy'\\n    \\ntrain=np.load(train_file)\\nvalid=np.load(valid_file)\\n#test=np.load(test_file)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_file = 'timit_train_subset_1ok.npy'\n",
    "valid_file = 'timit_valid_1ok.npy'\n",
    "#test_file = 'test_1ok.npy'\n",
    "    \n",
    "train=np.load(train_file)\n",
    "valid=np.load(valid_file)\n",
    "#test=np.load(test_file)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(np.where(train[2][1] == 1))\\nprint(train[1][1])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(np.where(train[2][1] == 1))\n",
    "print(train[1][1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ni=40\\nplt.figure(1)\\nplt.imshow(train[0][i].reshape(11,40).transpose(), extent=[0,11,0,40])\\nprint train[1][i]\\nplt.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "i=40\n",
    "plt.figure(1)\n",
    "plt.imshow(train[0][i].reshape(11,40).transpose(), extent=[0,11,0,40])\n",
    "print train[1][i]\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set\n",
      "Loading valid set\n",
      "number of minibatch at one epoch: train  1191, validation 186\n"
     ]
    }
   ],
   "source": [
    "'''Load Data'''\n",
    "train_file = 'timit_train_subset_1ok.npy'\n",
    "valid_file = 'timit_valid_1ok.npy'\n",
    "#test_file = 'test_1ok.npy'\n",
    "    \n",
    "train=np.load(train_file)\n",
    "valid=np.load(valid_file)\n",
    "#test=np.load(test_file)\n",
    "\n",
    "    \n",
    "\n",
    "print('Loading training set')    \n",
    "train_feat, train_label = util.shared_dataset_timit(train)\n",
    "print('Loading valid set')\n",
    "valid_feat, valid_label = util.shared_dataset_timit(valid)\n",
    "#test_feat, test_label = util.shared_dataset(test) \n",
    "    \n",
    "  \n",
    "'''Coefficient Initial'''        \n",
    "batch_size = 300\n",
    "n_epochs = 5\n",
    "learning_rate = 0.1\n",
    "    \n",
    "n_train_batches = train_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "#n_test_batches = test_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "print('number of minibatch at one epoch: train  %i, validation %i' %\n",
    "    (n_train_batches, n_valid_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357376\n",
      "357376\n",
      "440\n",
      "1984\n"
     ]
    }
   ],
   "source": [
    "z_dim = 10 #dimension of latent variable \n",
    "x_dim = train_feat.get_value(borrow=True).shape[1]\n",
    "y_dim = train_label.get_value(borrow=True).shape[1]\n",
    "activation = None\n",
    "    \n",
    "print(train_feat.get_value(borrow=True).shape[0])\n",
    "print(train_label.get_value(borrow=True).shape[0])\n",
    "print(train_feat.get_value(borrow=True).shape[1])\n",
    "print(train_label.get_value(borrow=True).shape[1])\n",
    "\n",
    "phi_1_struct=nn.NN_struct()\n",
    "phi_1_struct.layer_dim = [x_dim+y_dim, 500, z_dim]\n",
    "phi_1_struct.activation = [None, None]\n",
    "    \n",
    "theta_1_struct=nn.NN_struct()\n",
    "theta_1_struct.layer_dim = [x_dim, 500, z_dim]\n",
    "theta_1_struct.activation = [None, None]\n",
    "\n",
    "theta_2_struct=nn.NN_struct()\n",
    "theta_2_struct.layer_dim = [z_dim+x_dim, 500, y_dim]\n",
    "theta_2_struct.activation = [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()  # index to a [mini]batch\n",
    "x = T.matrix('x')  # the data is presented as rasterized images\n",
    "y = T.matrix('y')  # the labels are presented as signal vector   \n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "#with open('model.save', 'rb') as f:\n",
    "#    model = cPickle.load(f)\n",
    "\n",
    "\n",
    "#model.input_x=x\n",
    "#model.label_y=x\n",
    "#model.phi_mu.OL.W.get_value()\n",
    "#test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()  # index to a [mini]batch\n",
    "x = T.matrix('x')  # the data is presented as rasterized images\n",
    "y = T.matrix('y')  # the labels are presented as signal vector   \n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "#with open('model.save', 'rb') as f:\n",
    "#    model = cPickle.load(f)\n",
    "\n",
    "\n",
    "#model.input_x=x\n",
    "#model.label_y=x\n",
    "#model.phi_mu.OL.W.get_value()\n",
    "#test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = SVAE.Supervised_VAE_v3_CE(\n",
    "    rng=rng,\n",
    "    input_x = x,\n",
    "    label_y = y,\n",
    "    batch_size = batch_size,\n",
    "    phi_1_struct = phi_1_struct,\n",
    "    theta_2_struct = theta_2_struct,\n",
    "    in_dim = x_dim,\n",
    "    out_dim = y_dim\n",
    "    )\n",
    "\n",
    "'''\n",
    "cost = (classifier.cost)\n",
    "        \n",
    "gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "                   \n",
    "updates = [\n",
    "    (param, param - learning_rate * gparam)\n",
    "    for param, gparam in zip(classifier.params, gparams)\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "updates = update.adam(loss=classifier.cost, all_params=classifier.params, learning_rate=0.0005)\n",
    "\n",
    "\n",
    "#classifier.theta_mu = model.phi_mu\n",
    "#classifier.theta_sigma = model.phi_sigma\n",
    "\n",
    "#print(classifier.theta_mu.OL.W.get_value())\n",
    "#print(model.phi_mu.OL.W.get_value())\n",
    "#print(parp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... prepare training model\n",
      "... prepare validate model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('... prepare test model')\\ntest_model = theano.function(\\n    inputs=[index],\\n    outputs=[classifier.predictor, classifier.label_y],\\n    givens={\\n        x: test_feat[index * batch_size : (index + 1) * batch_size, :],\\n        y: test_label[index * batch_size : (index + 1) * batch_size, :]\\n    }        \\n)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('... prepare training model')\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier.cost, classifier.predictor, classifier.label_y],\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: train_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }       \n",
    ")   \n",
    "    \n",
    "print('... prepare validate model')\n",
    "validate_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier.cost,\n",
    "    givens={\n",
    "        x: valid_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: valid_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")      \n",
    "\n",
    "'''\n",
    "print('... prepare test model')\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier.predictor, classifier.label_y],\n",
    "    givens={\n",
    "        x: test_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: test_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "minibatch 0/1191\n",
      "minibatch 100/1191\n",
      "minibatch 200/1191\n",
      "minibatch 300/1191\n",
      "minibatch 400/1191\n",
      "minibatch 500/1191\n",
      "minibatch 600/1191\n",
      "minibatch 700/1191\n",
      "minibatch 800/1191\n",
      "minibatch 900/1191\n",
      "minibatch 1000/1191\n",
      "minibatch 1100/1191\n",
      "epoch training accuracy: 0.308805, training loss: 1.291277\n",
      "epoch 1, minibatch 1191/1191, validation loss 1.470716\n",
      "minibatch 9/1191\n",
      "minibatch 109/1191\n",
      "minibatch 209/1191\n",
      "minibatch 309/1191\n",
      "minibatch 409/1191\n",
      "minibatch 509/1191\n",
      "minibatch 609/1191\n",
      "minibatch 709/1191\n",
      "minibatch 809/1191\n",
      "minibatch 909/1191\n",
      "minibatch 1009/1191\n",
      "minibatch 1109/1191\n",
      "epoch training accuracy: 0.894545, training loss: 0.088603\n",
      "epoch 2, minibatch 1191/1191, validation loss 0.166461\n",
      "minibatch 18/1191\n",
      "minibatch 118/1191\n",
      "minibatch 218/1191\n",
      "minibatch 318/1191\n",
      "minibatch 418/1191\n",
      "minibatch 518/1191\n",
      "minibatch 618/1191\n",
      "minibatch 718/1191\n",
      "minibatch 818/1191\n",
      "minibatch 918/1191\n",
      "minibatch 1018/1191\n",
      "minibatch 1118/1191\n",
      "epoch training accuracy: 0.990912, training loss: 0.017033\n",
      "epoch 3, minibatch 1191/1191, validation loss 0.048195\n",
      "minibatch 27/1191\n",
      "minibatch 127/1191\n",
      "minibatch 227/1191\n",
      "minibatch 327/1191\n",
      "minibatch 427/1191\n",
      "minibatch 527/1191\n",
      "minibatch 627/1191\n",
      "minibatch 727/1191\n",
      "minibatch 827/1191\n",
      "minibatch 927/1191\n",
      "minibatch 1027/1191\n",
      "minibatch 1127/1191\n",
      "epoch training accuracy: 0.992592, training loss: 0.004948\n",
      "epoch 4, minibatch 1191/1191, validation loss 0.019051\n",
      "minibatch 36/1191\n",
      "minibatch 136/1191\n",
      "minibatch 236/1191\n",
      "minibatch 336/1191\n",
      "minibatch 436/1191\n",
      "minibatch 536/1191\n",
      "minibatch 636/1191\n",
      "minibatch 736/1191\n",
      "minibatch 836/1191\n",
      "minibatch 936/1191\n",
      "minibatch 1036/1191\n",
      "minibatch 1136/1191\n",
      "epoch training accuracy: 0.999731, training loss: 0.001922\n",
      "epoch 5, minibatch 1191/1191, validation loss 0.010140\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "'''\n",
    "Define :\n",
    "    xx_loss : Cost function value\n",
    "    xx_score : Classification accuracy rate\n",
    "'''        \n",
    "    \n",
    "print('... training')\n",
    "    \n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    \n",
    "#validation_frequency = n_train_batches\n",
    "    \n",
    "best_iter = 0\n",
    "best_train_loss = np.inf\n",
    "best_validation_loss = np.inf  \n",
    "test_loss = np.inf\n",
    "train_score = 0.\n",
    "validation_score = 0.\n",
    "test_score = 0.    \n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    train_acc=[]\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        [minibatch_avg_cost, pred, lab] \\\n",
    "        = train_model(minibatch_index)\n",
    "        #print(minibatch_index)\n",
    "        #print(pred)               \n",
    "        # iteration number\n",
    "        #print('minibatch %i/%i' % (minibatch_index, n_train_batches))\n",
    "        \n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        if iter % 100 == 0:\n",
    "            print('minibatch %i/%i' % (minibatch_index, n_train_batches))\n",
    "            \n",
    "        train_acc.append(get_acc(pred, np.nonzero(lab)[1]))\n",
    "            \n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in range(n_valid_batches)] \n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "                \n",
    "            #print('CE loss: %f' % (np.mean(KL_loss)))\n",
    "            #print('encoder mu: %f, sigma: %f' % (ec_m, ec_s))\n",
    "            #print('decoder mu: %f, sigma: %f' % (dc_m, dc_s))\n",
    "            #print(ec_m)\n",
    "            #print(dc_m)\n",
    "            #print(ec_s)\n",
    "            #print(dc_s)\n",
    "            \n",
    "            print('epoch training accuracy: %f, training loss: %f' \\\n",
    "                % (np.mean(np.array(train_acc)), np.mean(minibatch_avg_cost)))\n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, validation loss %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (\n",
    "                    this_validation_loss < best_validation_loss *\n",
    "                    improvement_threshold\n",
    "                ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss   \n",
    "                best_iter = iter\n",
    "                '''\n",
    "                # get training accuracy\n",
    "                print('best training accuracy: %f' % (np.mean(np.array(train_acc))))\n",
    "                # test it on the test set\n",
    "                #test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                #test_score = np.mean(test_losses)\n",
    "\n",
    "                print(('epoch %i, minibatch %i/%i, best train accuracy: %f') % \\\n",
    "                        (epoch, minibatch_index + 1, n_train_batches, \\\n",
    "                        np.mean(np.array(train_acc))))\n",
    "                '''\n",
    "                \n",
    "                '''\n",
    "                test_acc=[]\n",
    "                for minibatch_index in range(n_test_batches):\n",
    "                    [pred_test, lab_test]= test_model(minibatch_index)\n",
    "                    #print(pred)\n",
    "                    #print(np.nonzero(lab)[1])\n",
    "                    # iteration number\n",
    "                    iter = minibatch_index\n",
    "                    test_acc.append(get_acc(pred_test, np.nonzero(lab_test)[1]))\n",
    "    \n",
    "                print('test accuracy: %f' % (np.mean(test_acc)))\n",
    "                '''\n",
    "                \n",
    "\n",
    "        if patience <= iter:\n",
    "            done_loop\n",
    "            ing = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... prepare test model\n"
     ]
    }
   ],
   "source": [
    "print('... prepare test model')\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier.predictor, classifier.label_y, classifier.PR_y_pred],\n",
    "    givens={\n",
    "        x: valid_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: valid_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#validation_losses = [validate_model(i) for i in range(n_valid_batches)]\n",
    "[pred, y, pr_y_pred] = test_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08598345,  0.04936071, -0.00484833, ...,  0.11585402,\n",
       "        -0.12958887,  0.05190004],\n",
       "       [ 0.16762261,  0.0967848 ,  0.05059614, ..., -0.12501037,\n",
       "         0.01127056,  0.0035666 ],\n",
       "       [-0.00532561,  0.17155547,  0.04719282, ..., -0.11836273,\n",
       "        -0.04804441,  0.03040252],\n",
       "       ..., \n",
       "       [-0.19622377,  0.1814644 ,  0.21150714, ..., -0.13161674,\n",
       "         0.0925743 , -0.00544427],\n",
       "       [-0.04907462, -0.03964802, -0.18481621, ..., -0.34770074,\n",
       "         0.41454947, -0.09753846],\n",
       "       [-0.57451648,  0.66295689, -0.2089753 , ..., -0.63351285,\n",
       "         0.37848681, -0.50703043]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.phi_mu.HL_1.W.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('timit_model_step_ce_1l_500h_z10.save', 'wb') as f:\n",
    "    cPickle.dump(classifier, f, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaa=classifier.phi_mu.HL_1.W.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
