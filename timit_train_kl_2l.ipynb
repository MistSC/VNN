{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from six.moves import cPickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import nnet as nn\n",
    "import criteria as er\n",
    "import util\n",
    "import VAE\n",
    "import SVAE\n",
    "import update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_acc(pred, true):\n",
    "    ll = pred - true\n",
    "    ll = np.array(ll)\n",
    "    acc = 1 - (np.nonzero(ll)[0].shape[0])/float(ll.shape[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Load Data'''\n",
    "train_file = 'timit_train_subset_1ok.npy'\n",
    "valid_file = 'timit_valid_1ok.npy'\n",
    "#test_file = 'test_1ok.npy'\n",
    "    \n",
    "train=np.load(train_file)\n",
    "valid=np.load(valid_file)\n",
    "#test=np.load(test_file)\n",
    "\n",
    "    \n",
    "\n",
    "print('Loading training set')    \n",
    "train_feat, train_label = util.shared_dataset_timit(train)\n",
    "print('Loading valida')\n",
    "valid_feat, valid_label = util.shared_dataset_timit(valid)\n",
    "#test_feat, test_label = util.shared_dataset(test) \n",
    "    \n",
    "  \n",
    "'''Coefficient Initial'''        \n",
    "batch_size = 300\n",
    "n_epochs = 5\n",
    "    \n",
    "n_train_batches = train_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "#n_test_batches = test_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "print('number of minibatch at one epoch: train  %i, validation %i' %\n",
    "    (n_train_batches, n_valid_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label.get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_dim = 5 #dimension of latent variable \n",
    "x_dim = train_feat.get_value(borrow=True).shape[1]\n",
    "y_dim = train_label.get_value(borrow=True).shape[1]\n",
    "activation = None\n",
    "    \n",
    "print(train_feat.get_value(borrow=True).shape[0])\n",
    "print(train_label.get_value(borrow=True).shape[0])\n",
    "print(train_feat.get_value(borrow=True).shape[1])\n",
    "print(train_label.get_value(borrow=True).shape[1])\n",
    "\n",
    "\n",
    "phi_1_struct=nn.NN_struct()\n",
    "phi_1_struct.layer_dim = [x_dim+y_dim, 500, 500, z_dim]\n",
    "phi_1_struct.activation = [None, None, None]\n",
    "    \n",
    "theta_1_struct=nn.NN_struct()\n",
    "theta_1_struct.layer_dim = [x_dim, 500, 500, z_dim]\n",
    "theta_1_struct.activation = [None, None, None]\n",
    "\n",
    "theta_2_struct=nn.NN_struct()\n",
    "theta_2_struct.layer_dim = [z_dim+x_dim, 500, 500, y_dim]\n",
    "theta_2_struct.activation = [None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()  # index to a [mini]batch\n",
    "x = T.matrix('x')  # the data is presented as rasterized images\n",
    "y = T.matrix('y')  # the labels are presented as signal vector   \n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "with open('timit_model_step_ce_2l_500h_z5.save', 'rb') as f:\n",
    "    model = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.phi_mu.HL_1.W.get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_kl = SVAE.Supervised_VAE_v3_KL_2L(\n",
    "    rng=rng,\n",
    "    input_x = x,\n",
    "    label_y = y,\n",
    "    batch_size = batch_size,\n",
    "    phi_1_struct = phi_1_struct,\n",
    "    theta_1_struct = theta_1_struct,\n",
    "    theta_2_struct = theta_2_struct,\n",
    "    in_dim = x_dim,\n",
    "    out_dim = y_dim,\n",
    "    model = model\n",
    "    )\n",
    "\n",
    "\n",
    "'''\n",
    "learning_rate = 0.008\n",
    "cost = (classifier_kl.cost)\n",
    "gparams = [T.grad(cost, param) for param in classifier_kl.params]\n",
    "updates = [\n",
    "    (param, param - learning_rate * gparam)\n",
    "    for param, gparam in zip(classifier_kl.params, gparams)\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "updates = update.adam(loss=classifier_kl.cost, all_params=classifier_kl.params, learning_rate=0.001)\n",
    "\n",
    "#classifier_kl.phi_mu = model.phi_mu\n",
    "#classifier_kl.phi_sigma = model.phi_sigma\n",
    "#classifier_kl.theta_2 = model.theta_2\n",
    "#classifier_kl.predict = model.predict\n",
    "#print(classifier_kl.phi_sigma.OL.W.get_value())\n",
    "#print(model.phi_sigma.OL.W.get_value())\n",
    "#print(classifier_kl.theta_2.OL.W.get_value())\n",
    "#print(model.theta_2.OL.W.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('... prepare training model')\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier_kl.cost, classifier_kl.predictor, classifier_kl.label_y, classifier_kl.predictor_test,\n",
    "             classifier_kl.EC_mu, classifier_kl.EC_log_sigma, classifier_kl.DC_mu, classifier_kl.DC_log_sigma],\n",
    "            #classifier_kl.KL, classifier_kl.CE],\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: train_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }       \n",
    ")   \n",
    "    \n",
    "print('... prepare validate model')\n",
    "validate_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier_kl.cost,\n",
    "    givens={\n",
    "        x: valid_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: valid_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")      \n",
    "\n",
    "'''\n",
    "print('... prepare test model')\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier_kl.predictor_test, classifier_kl.label_y],\n",
    "    givens={\n",
    "        x: test_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: test_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "'''\n",
    "Define :\n",
    "    xx_loss : Cost function value\n",
    "    xx_score : Classification accuracy rate\n",
    "'''        \n",
    "    \n",
    "print('... training')\n",
    "    \n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    \n",
    "#validation_frequency = n_train_batches\n",
    "    \n",
    "best_iter = 0\n",
    "best_train_loss = np.inf\n",
    "best_validation_loss = np.inf  \n",
    "test_loss = np.inf\n",
    "train_score = 0.\n",
    "validation_score = 0.\n",
    "test_score = 0.    \n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    train_acc1=[]\n",
    "    train_acc2=[]\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        [minibatch_avg_cost, pred1, lab, pred2, ec_mu, ec_log_sigma, dc_mu, dc_log_sigma] \\\n",
    "        = train_model(minibatch_index)\n",
    "        #print(minibatch_index)\n",
    "                        \n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        if iter % 100 == 0:\n",
    "            print('minibatch %i/%i' % (minibatch_index, n_train_batches))\n",
    "        #print(pred1)\n",
    "        #print(classifier_kl.phi_sigma.OL.W.get_value())\n",
    "        train_acc1.append(get_acc(pred1, np.nonzero(lab)[1]))\n",
    "        train_acc2.append(get_acc(pred2, np.nonzero(lab)[1]))\n",
    "            \n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in range(n_valid_batches)] \n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "                \n",
    "            #print('CE loss: %f' % (np.mean(KL_loss)))\n",
    "            #print('encoder mu: %f, sigma: %f' % (ec_m, ec_s))\n",
    "            #print('decoder mu: %f, sigma: %f' % (dc_m, dc_s))\n",
    "            #print(ec_m)\n",
    "            #print(dc_m)\n",
    "            #print(ec_s)\n",
    "            #print(dc_s)\n",
    "            \n",
    "            #print('epoch training accuracy ec: %f, dc: %f, training loss: %f \\n trainig kl: %f, ce: %f' \\\n",
    "            #    % (np.mean(np.array(train_acc1)), np.mean(np.array(train_acc2)),np.mean(minibatch_avg_cost), \\\n",
    "            #      np.mean(kl), np.mean(ce)))\n",
    "            print('encoder mean: %r \\n encoder log variance: %r \\n decoder mean: %r \\n decoder log variance: %r \\n' \\\n",
    "                  % (ec_mu, ec_log_sigma, dc_mu, dc_log_sigma))\n",
    "            print('epoch training accuracy ec: %f, dc: %f, training loss: %f' \\\n",
    "                % (np.mean(np.array(train_acc1)), np.mean(np.array(train_acc2)),np.mean(minibatch_avg_cost)))\n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, validation loss %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (\n",
    "                    this_validation_loss < best_validation_loss *\n",
    "                    improvement_threshold\n",
    "                ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss   \n",
    "                best_iter = iter\n",
    "                \n",
    "                '''\n",
    "                # get training accuracy\n",
    "                print('best training accuracy: %f' % (np.mean(np.array(train_acc))))\n",
    "                # test it on the test set\n",
    "                #test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                #test_score = np.mean(test_losses)\n",
    "\n",
    "                print(('epoch %i, minibatch %i/%i, best train accuracy: %f') % \\\n",
    "                        (epoch, minibatch_index + 1, n_train_batches, \\\n",
    "                        np.mean(np.array(train_acc))))\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                \n",
    "                '''\n",
    "                test_acc=[]\n",
    "                for minibatch_index in range(n_test_batches):\n",
    "                    [pred_test, lab_test]= test_model(minibatch_index)\n",
    "                    #print(pred)\n",
    "                    #print(np.nonzero(lab)[1])\n",
    "                    # iteration number\n",
    "                    iter = minibatch_index\n",
    "                    test_acc.append(get_acc(pred_test, np.nonzero(lab_test)[1]))\n",
    "    \n",
    "                print('test accuracy: %f' % (np.mean(test_acc)))\n",
    "                '''\n",
    "                \n",
    "\n",
    "        #if patience <= iter:\n",
    "        #    done_looping = True\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
