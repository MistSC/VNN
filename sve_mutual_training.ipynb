{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from six.moves import cPickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import nnet as nn\n",
    "import criteria as er\n",
    "import util\n",
    "import VAE\n",
    "import SVAE\n",
    "import update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_acc(pred, true):\n",
    "    ll = pred - true\n",
    "    ll = np.array(ll)\n",
    "    acc = 1 - (np.nonzero(ll)[0].shape[0])/float(ll.shape[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of minibatch at one epoch: train  183, validation 16, test 33\n",
      "55000\n",
      "55000\n",
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "'''Load Data'''\n",
    "train_file = 'train_1ok.npy'\n",
    "valid_file = 'valid_1ok.npy'\n",
    "test_file = 'test_1ok.npy'\n",
    "    \n",
    "train=np.load(train_file)\n",
    "valid=np.load(valid_file)\n",
    "test=np.load(test_file)\n",
    "\n",
    "#train_list=np.load('train.npy')[1]\n",
    "    \n",
    "\n",
    "train_feat, train_label = util.shared_dataset(train)\n",
    "valid_feat, valid_label = util.shared_dataset(valid)\n",
    "test_feat, test_label = util.shared_dataset(test) \n",
    "    \n",
    "  \n",
    "'''Coefficient Initial'''        \n",
    "batch_size = 300\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "    \n",
    "n_train_batches = train_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_test_batches = test_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "print('number of minibatch at one epoch: train  %i, validation %i, test %i' %\n",
    "    (n_train_batches, n_valid_batches, n_test_batches))\n",
    "    \n",
    "z_dim = 5 #dimension of latent variable \n",
    "x_dim = train_feat.get_value(borrow=True).shape[1]\n",
    "y_dim = train_label.get_value(borrow=True).shape[1]\n",
    "activation = None\n",
    "    \n",
    "print(train_feat.get_value(borrow=True).shape[0])\n",
    "print(train_label.get_value(borrow=True).shape[0])\n",
    "print(train_feat.get_value(borrow=True).shape[1])\n",
    "print(train_label.get_value(borrow=True).shape[1])\n",
    "\n",
    "phi_1_struct=nn.NN_struct()\n",
    "phi_1_struct.layer_dim = [x_dim+y_dim, z_dim]\n",
    "phi_1_struct.activation = [activation]\n",
    "    \n",
    "theta_1_struct=nn.NN_struct()\n",
    "theta_1_struct.layer_dim = [x_dim, z_dim]\n",
    "theta_1_struct.activation = [activation]\n",
    "\n",
    "theta_2_struct=nn.NN_struct()\n",
    "theta_2_struct.layer_dim = [z_dim+x_dim, y_dim]\n",
    "theta_2_struct.activation = [activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()  # index to a [mini]batch\n",
    "x = T.matrix('x')  # the data is presented as rasterized images\n",
    "y = T.matrix('y')  # the labels are presented as signal vector   \n",
    "xx = T.matrix('xx')  # the data is presented as rasterized images\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "with open('model.save', 'rb') as f:\n",
    "    model = cPickle.load(f)\n",
    "\n",
    "\n",
    "#model.input_x=x\n",
    "#model.label_y=x\n",
    "#model.phi_mu.OL.W.get_value()\n",
    "#test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = SVAE.Supervised_VAE_v3(\n",
    "    rng=rng,\n",
    "    input_x = x,\n",
    "    label_y = y,\n",
    "    batch_size = batch_size,\n",
    "    phi_1_struct = phi_1_struct,\n",
    "    theta_1_struct = theta_1_struct,\n",
    "    theta_2_struct = theta_2_struct,\n",
    "    in_dim = x_dim,\n",
    "    out_dim = y_dim,\n",
    "    model = model\n",
    "    )\n",
    "'''\n",
    "cost = (classifier.cost)\n",
    "        \n",
    "gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "                   \n",
    "updates = [\n",
    "    (param, param - learning_rate * gparam)\n",
    "    for param, gparam in zip(classifier.params, gparams)\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "updates = update.adam(loss=classifier.cost, all_params=classifier.params, learning_rate=0.001)\n",
    "#classifier.theta_mu = model.phi_mu\n",
    "#classifier.theta_sigma = model.phi_sigma\n",
    "\n",
    "#print(classifier.theta_mu.OL.W.get_value())\n",
    "#print(model.phi_mu.OL.W.get_value())\n",
    "#print(parp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... prepare training model\n",
      "... prepare validate model\n",
      "... prepare test model\n"
     ]
    }
   ],
   "source": [
    "print('... prepare training model')\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier.cost, classifier.KL, classifier.CE, classifier.predictor, classifier.label_y, \\\n",
    "            classifier.predictor_test, classifier.PR_y_pred, classifier.PR_test, \\\n",
    "            classifier.EC_mu, classifier.EC_sigma, classifier.DC_mu, classifier.DC_sigma],\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: train_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }       \n",
    ")   \n",
    "    \n",
    "    \n",
    "print('... prepare validate model')\n",
    "validate_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier.cost,\n",
    "    givens={\n",
    "        x: valid_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: valid_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")      \n",
    "\n",
    "print('... prepare test model')\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier.predictor_test, classifier.label_y],\n",
    "    givens={\n",
    "        x: test_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: test_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "KL loss: -0.355412, CE loss: 1.741921\n",
      "epoch training accuracy: 0.541129, 0.098743\n",
      "epoch 1, minibatch 183/183, validation loss 2.080796\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.139596, CE loss: 1.297270\n",
      "epoch training accuracy: 0.714208, 0.098743\n",
      "epoch 2, minibatch 183/183, validation loss 1.438575\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.077168, CE loss: 1.017819\n",
      "epoch training accuracy: 0.786648, 0.098743\n",
      "epoch 3, minibatch 183/183, validation loss 1.097923\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.046797, CE loss: 0.821084\n",
      "epoch training accuracy: 0.829909, 0.098743\n",
      "epoch 4, minibatch 183/183, validation loss 0.876299\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.032490, CE loss: 0.682785\n",
      "epoch training accuracy: 0.851949, 0.098743\n",
      "epoch 5, minibatch 183/183, validation loss 0.725696\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.022642, CE loss: 0.585073\n",
      "epoch training accuracy: 0.866539, 0.098743\n",
      "epoch 6, minibatch 183/183, validation loss 0.618547\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.017096, CE loss: 0.512933\n",
      "epoch training accuracy: 0.874991, 0.098743\n",
      "epoch 7, minibatch 183/183, validation loss 0.543720\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.012924, CE loss: 0.459033\n",
      "epoch training accuracy: 0.880783, 0.098743\n",
      "epoch 8, minibatch 183/183, validation loss 0.486100\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.009337, CE loss: 0.415761\n",
      "epoch training accuracy: 0.885574, 0.098743\n",
      "epoch 9, minibatch 183/183, validation loss 0.444180\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.009202, CE loss: 0.385649\n",
      "epoch training accuracy: 0.888852, 0.098743\n",
      "epoch 10, minibatch 183/183, validation loss 0.413945\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.006072, CE loss: 0.360398\n",
      "epoch training accuracy: 0.891457, 0.098743\n",
      "epoch 11, minibatch 183/183, validation loss 0.385822\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.004517, CE loss: 0.336236\n",
      "epoch training accuracy: 0.894098, 0.098743\n",
      "epoch 12, minibatch 183/183, validation loss 0.364426\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.003906, CE loss: 0.320543\n",
      "epoch training accuracy: 0.896667, 0.098743\n",
      "epoch 13, minibatch 183/183, validation loss 0.347084\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.003139, CE loss: 0.305391\n",
      "epoch training accuracy: 0.898907, 0.098743\n",
      "epoch 14, minibatch 183/183, validation loss 0.332278\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.003385, CE loss: 0.293961\n",
      "epoch training accuracy: 0.900291, 0.098743\n",
      "epoch 15, minibatch 183/183, validation loss 0.320961\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.002440, CE loss: 0.281691\n",
      "epoch training accuracy: 0.902168, 0.098743\n",
      "epoch 16, minibatch 183/183, validation loss 0.310294\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.002123, CE loss: 0.268662\n",
      "epoch training accuracy: 0.903679, 0.098743\n",
      "epoch 17, minibatch 183/183, validation loss 0.301043\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.003201, CE loss: 0.263152\n",
      "epoch training accuracy: 0.905464, 0.098743\n",
      "epoch 18, minibatch 183/183, validation loss 0.293839\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.002099, CE loss: 0.256431\n",
      "epoch training accuracy: 0.906740, 0.098743\n",
      "epoch 19, minibatch 183/183, validation loss 0.286562\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001569, CE loss: 0.249367\n",
      "epoch training accuracy: 0.907851, 0.098743\n",
      "epoch 20, minibatch 183/183, validation loss 0.279177\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001845, CE loss: 0.243001\n",
      "epoch training accuracy: 0.909800, 0.098743\n",
      "epoch 21, minibatch 183/183, validation loss 0.273880\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001367, CE loss: 0.238066\n",
      "epoch training accuracy: 0.910692, 0.098743\n",
      "epoch 22, minibatch 183/183, validation loss 0.268993\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001079, CE loss: 0.235450\n",
      "epoch training accuracy: 0.912295, 0.098743\n",
      "epoch 23, minibatch 183/183, validation loss 0.264141\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001193, CE loss: 0.231362\n",
      "epoch training accuracy: 0.913078, 0.098743\n",
      "epoch 24, minibatch 183/183, validation loss 0.259766\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000844, CE loss: 0.226712\n",
      "epoch training accuracy: 0.914026, 0.098743\n",
      "epoch 25, minibatch 183/183, validation loss 0.256365\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001424, CE loss: 0.222836\n",
      "epoch training accuracy: 0.915100, 0.098743\n",
      "epoch 26, minibatch 183/183, validation loss 0.253861\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000754, CE loss: 0.219322\n",
      "epoch training accuracy: 0.916375, 0.098743\n",
      "epoch 27, minibatch 183/183, validation loss 0.249643\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000792, CE loss: 0.215585\n",
      "epoch training accuracy: 0.916831, 0.098743\n",
      "epoch 28, minibatch 183/183, validation loss 0.246797\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.001061, CE loss: 0.216305\n",
      "epoch training accuracy: 0.918033, 0.098743\n",
      "epoch 29, minibatch 183/183, validation loss 0.244444\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000852, CE loss: 0.214531\n",
      "epoch training accuracy: 0.918962, 0.098743\n",
      "epoch 30, minibatch 183/183, validation loss 0.241275\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000694, CE loss: 0.210453\n",
      "epoch training accuracy: 0.919326, 0.098743\n",
      "epoch 31, minibatch 183/183, validation loss 0.239119\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000774, CE loss: 0.211003\n",
      "epoch training accuracy: 0.920164, 0.098743\n",
      "epoch 32, minibatch 183/183, validation loss 0.237244\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000691, CE loss: 0.203978\n",
      "epoch training accuracy: 0.921075, 0.098743\n",
      "epoch 33, minibatch 183/183, validation loss 0.235439\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000518, CE loss: 0.202324\n",
      "epoch training accuracy: 0.921840, 0.098743\n",
      "epoch 34, minibatch 183/183, validation loss 0.233505\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000659, CE loss: 0.201054\n",
      "epoch training accuracy: 0.922514, 0.098743\n",
      "epoch 35, minibatch 183/183, validation loss 0.231980\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000464, CE loss: 0.202133\n",
      "epoch training accuracy: 0.923461, 0.098743\n",
      "epoch 36, minibatch 183/183, validation loss 0.229974\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000406, CE loss: 0.199952\n",
      "epoch training accuracy: 0.923534, 0.098743\n",
      "epoch 37, minibatch 183/183, validation loss 0.227921\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000361, CE loss: 0.195572\n",
      "epoch training accuracy: 0.924317, 0.098743\n",
      "epoch 38, minibatch 183/183, validation loss 0.228017\n",
      "KL loss: -0.000344, CE loss: 0.196704\n",
      "epoch training accuracy: 0.924882, 0.098743\n",
      "epoch 39, minibatch 183/183, validation loss 0.226241\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000352, CE loss: 0.191943\n",
      "epoch training accuracy: 0.925373, 0.098743\n",
      "epoch 40, minibatch 183/183, validation loss 0.224812\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000293, CE loss: 0.191342\n",
      "epoch training accuracy: 0.925865, 0.098743\n",
      "epoch 41, minibatch 183/183, validation loss 0.223188\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000262, CE loss: 0.188785\n",
      "epoch training accuracy: 0.926521, 0.098743\n",
      "epoch 42, minibatch 183/183, validation loss 0.221855\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000290, CE loss: 0.191061\n",
      "epoch training accuracy: 0.927049, 0.098743\n",
      "epoch 43, minibatch 183/183, validation loss 0.221016\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000302, CE loss: 0.188283\n",
      "epoch training accuracy: 0.927687, 0.098743\n",
      "epoch 44, minibatch 183/183, validation loss 0.219909\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000302, CE loss: 0.186966\n",
      "epoch training accuracy: 0.928597, 0.098743\n",
      "epoch 45, minibatch 183/183, validation loss 0.218599\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000266, CE loss: 0.183352\n",
      "epoch training accuracy: 0.928561, 0.098743\n",
      "epoch 46, minibatch 183/183, validation loss 0.218275\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000282, CE loss: 0.184472\n",
      "epoch training accuracy: 0.929162, 0.098743\n",
      "epoch 47, minibatch 183/183, validation loss 0.216627\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000258, CE loss: 0.183129\n",
      "epoch training accuracy: 0.929599, 0.098743\n",
      "epoch 48, minibatch 183/183, validation loss 0.215805\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000215, CE loss: 0.182042\n",
      "epoch training accuracy: 0.930146, 0.098743\n",
      "epoch 49, minibatch 183/183, validation loss 0.215139\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000206, CE loss: 0.181619\n",
      "epoch training accuracy: 0.930528, 0.098743\n",
      "epoch 50, minibatch 183/183, validation loss 0.214982\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000225, CE loss: 0.182097\n",
      "epoch training accuracy: 0.930747, 0.098743\n",
      "epoch 51, minibatch 183/183, validation loss 0.213367\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000270, CE loss: 0.182666\n",
      "epoch training accuracy: 0.931403, 0.098743\n",
      "epoch 52, minibatch 183/183, validation loss 0.213373\n",
      "KL loss: -0.000202, CE loss: 0.181843\n",
      "epoch training accuracy: 0.931457, 0.098743\n",
      "epoch 53, minibatch 183/183, validation loss 0.211909\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000222, CE loss: 0.181195\n",
      "epoch training accuracy: 0.931767, 0.098743\n",
      "epoch 54, minibatch 183/183, validation loss 0.211300\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000155, CE loss: 0.178532\n",
      "epoch training accuracy: 0.932259, 0.098743\n",
      "epoch 55, minibatch 183/183, validation loss 0.210815\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000147, CE loss: 0.180038\n",
      "epoch training accuracy: 0.932605, 0.098743\n",
      "epoch 56, minibatch 183/183, validation loss 0.210564\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000194, CE loss: 0.179534\n",
      "epoch training accuracy: 0.932896, 0.098743\n",
      "epoch 57, minibatch 183/183, validation loss 0.209853\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000168, CE loss: 0.178937\n",
      "epoch training accuracy: 0.933169, 0.098743\n",
      "epoch 58, minibatch 183/183, validation loss 0.208933\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000187, CE loss: 0.177619\n",
      "epoch training accuracy: 0.933716, 0.098743\n",
      "epoch 59, minibatch 183/183, validation loss 0.208067\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000180, CE loss: 0.178186\n",
      "epoch training accuracy: 0.933843, 0.098743\n",
      "epoch 60, minibatch 183/183, validation loss 0.208236\n",
      "KL loss: -0.000174, CE loss: 0.177616\n",
      "epoch training accuracy: 0.934262, 0.098743\n",
      "epoch 61, minibatch 183/183, validation loss 0.207399\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000185, CE loss: 0.176875\n",
      "epoch training accuracy: 0.934372, 0.098743\n",
      "epoch 62, minibatch 183/183, validation loss 0.206277\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000166, CE loss: 0.174695\n",
      "epoch training accuracy: 0.934517, 0.098743\n",
      "epoch 63, minibatch 183/183, validation loss 0.206548\n",
      "KL loss: -0.000196, CE loss: 0.176923\n",
      "epoch training accuracy: 0.934954, 0.098743\n",
      "epoch 64, minibatch 183/183, validation loss 0.205788\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000200, CE loss: 0.175473\n",
      "epoch training accuracy: 0.935228, 0.098743\n",
      "epoch 65, minibatch 183/183, validation loss 0.206140\n",
      "KL loss: -0.000142, CE loss: 0.174688\n",
      "epoch training accuracy: 0.935610, 0.098743\n",
      "epoch 66, minibatch 183/183, validation loss 0.205448\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000152, CE loss: 0.173950\n",
      "epoch training accuracy: 0.935647, 0.098743\n",
      "epoch 67, minibatch 183/183, validation loss 0.204719\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000183, CE loss: 0.173167\n",
      "epoch training accuracy: 0.936011, 0.098743\n",
      "epoch 68, minibatch 183/183, validation loss 0.204194\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000151, CE loss: 0.174722\n",
      "epoch training accuracy: 0.935920, 0.098743\n",
      "epoch 69, minibatch 183/183, validation loss 0.203188\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000167, CE loss: 0.173790\n",
      "epoch training accuracy: 0.936539, 0.098743\n",
      "epoch 70, minibatch 183/183, validation loss 0.203558\n",
      "KL loss: -0.000170, CE loss: 0.172375\n",
      "epoch training accuracy: 0.936339, 0.098743\n",
      "epoch 71, minibatch 183/183, validation loss 0.203755\n",
      "KL loss: -0.000147, CE loss: 0.176074\n",
      "epoch training accuracy: 0.936721, 0.098743\n",
      "epoch 72, minibatch 183/183, validation loss 0.203268\n",
      "KL loss: -0.000130, CE loss: 0.172955\n",
      "epoch training accuracy: 0.936995, 0.098743\n",
      "epoch 73, minibatch 183/183, validation loss 0.203316\n",
      "KL loss: -0.000215, CE loss: 0.172462\n",
      "epoch training accuracy: 0.937231, 0.098743\n",
      "epoch 74, minibatch 183/183, validation loss 0.202560\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000195, CE loss: 0.173914\n",
      "epoch training accuracy: 0.937304, 0.098743\n",
      "epoch 75, minibatch 183/183, validation loss 0.202257\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000160, CE loss: 0.171891\n",
      "epoch training accuracy: 0.937486, 0.098743\n",
      "epoch 76, minibatch 183/183, validation loss 0.202010\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000103, CE loss: 0.171433\n",
      "epoch training accuracy: 0.937505, 0.098743\n",
      "epoch 77, minibatch 183/183, validation loss 0.201462\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000161, CE loss: 0.171601\n",
      "epoch training accuracy: 0.937869, 0.098743\n",
      "epoch 78, minibatch 183/183, validation loss 0.201720\n",
      "KL loss: -0.000204, CE loss: 0.170077\n",
      "epoch training accuracy: 0.937832, 0.098743\n",
      "epoch 79, minibatch 183/183, validation loss 0.201781\n",
      "KL loss: -0.000162, CE loss: 0.171178\n",
      "epoch training accuracy: 0.937960, 0.098743\n",
      "epoch 80, minibatch 183/183, validation loss 0.201017\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000141, CE loss: 0.169945\n",
      "epoch training accuracy: 0.938087, 0.098743\n",
      "epoch 81, minibatch 183/183, validation loss 0.200824\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000096, CE loss: 0.168048\n",
      "epoch training accuracy: 0.938342, 0.098743\n",
      "epoch 82, minibatch 183/183, validation loss 0.200112\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000228, CE loss: 0.169585\n",
      "epoch training accuracy: 0.938434, 0.098743\n",
      "epoch 83, minibatch 183/183, validation loss 0.200692\n",
      "KL loss: -0.000092, CE loss: 0.170446\n",
      "epoch training accuracy: 0.938798, 0.098743\n",
      "epoch 84, minibatch 183/183, validation loss 0.200323\n",
      "KL loss: -0.000142, CE loss: 0.168255\n",
      "epoch training accuracy: 0.938852, 0.098743\n",
      "epoch 85, minibatch 183/183, validation loss 0.200580\n",
      "KL loss: -0.000137, CE loss: 0.169065\n",
      "epoch training accuracy: 0.939016, 0.098743\n",
      "epoch 86, minibatch 183/183, validation loss 0.200184\n",
      "KL loss: -0.000116, CE loss: 0.170182\n",
      "epoch training accuracy: 0.939490, 0.098743\n",
      "epoch 87, minibatch 183/183, validation loss 0.199423\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000175, CE loss: 0.169325\n",
      "epoch training accuracy: 0.939526, 0.098743\n",
      "epoch 88, minibatch 183/183, validation loss 0.199752\n",
      "KL loss: -0.000108, CE loss: 0.168759\n",
      "epoch training accuracy: 0.939326, 0.098743\n",
      "epoch 89, minibatch 183/183, validation loss 0.198847\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000111, CE loss: 0.167672\n",
      "epoch training accuracy: 0.939709, 0.098743\n",
      "epoch 90, minibatch 183/183, validation loss 0.199344\n",
      "KL loss: -0.000122, CE loss: 0.168618\n",
      "epoch training accuracy: 0.939800, 0.098743\n",
      "epoch 91, minibatch 183/183, validation loss 0.199744\n",
      "KL loss: -0.000187, CE loss: 0.169519\n",
      "epoch training accuracy: 0.940200, 0.098743\n",
      "epoch 92, minibatch 183/183, validation loss 0.199257\n",
      "KL loss: -0.000148, CE loss: 0.167788\n",
      "epoch training accuracy: 0.940255, 0.098743\n",
      "epoch 93, minibatch 183/183, validation loss 0.198923\n",
      "KL loss: -0.000126, CE loss: 0.167342\n",
      "epoch training accuracy: 0.940583, 0.098743\n",
      "epoch 94, minibatch 183/183, validation loss 0.198873\n",
      "KL loss: -0.000121, CE loss: 0.168421\n",
      "epoch training accuracy: 0.940383, 0.098743\n",
      "epoch 95, minibatch 183/183, validation loss 0.199145\n",
      "KL loss: -0.000097, CE loss: 0.167657\n",
      "epoch training accuracy: 0.940820, 0.098743\n",
      "epoch 96, minibatch 183/183, validation loss 0.198195\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000099, CE loss: 0.166682\n",
      "epoch training accuracy: 0.940674, 0.098743\n",
      "epoch 97, minibatch 183/183, validation loss 0.198403\n",
      "KL loss: -0.000072, CE loss: 0.165548\n",
      "epoch training accuracy: 0.941148, 0.098743\n",
      "epoch 98, minibatch 183/183, validation loss 0.198373\n",
      "KL loss: -0.000094, CE loss: 0.165854\n",
      "epoch training accuracy: 0.941202, 0.098743\n",
      "epoch 99, minibatch 183/183, validation loss 0.197696\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000069, CE loss: 0.166449\n",
      "epoch training accuracy: 0.941512, 0.098743\n",
      "epoch 100, minibatch 183/183, validation loss 0.198022\n",
      "KL loss: -0.000080, CE loss: 0.165970\n",
      "epoch training accuracy: 0.941603, 0.098743\n",
      "epoch 101, minibatch 183/183, validation loss 0.197880\n",
      "KL loss: -0.000138, CE loss: 0.166218\n",
      "epoch training accuracy: 0.941967, 0.098743\n",
      "epoch 102, minibatch 183/183, validation loss 0.197733\n",
      "KL loss: -0.000076, CE loss: 0.165273\n",
      "epoch training accuracy: 0.941821, 0.098743\n",
      "epoch 103, minibatch 183/183, validation loss 0.197966\n",
      "KL loss: -0.000073, CE loss: 0.164941\n",
      "epoch training accuracy: 0.941985, 0.098743\n",
      "epoch 104, minibatch 183/183, validation loss 0.197523\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000168, CE loss: 0.166428\n",
      "epoch training accuracy: 0.942222, 0.098743\n",
      "epoch 105, minibatch 183/183, validation loss 0.197723\n",
      "KL loss: -0.000080, CE loss: 0.163757\n",
      "epoch training accuracy: 0.942514, 0.098743\n",
      "epoch 106, minibatch 183/183, validation loss 0.197581\n",
      "KL loss: -0.000075, CE loss: 0.164780\n",
      "epoch training accuracy: 0.942750, 0.098743\n",
      "epoch 107, minibatch 183/183, validation loss 0.197401\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000066, CE loss: 0.163553\n",
      "epoch training accuracy: 0.942750, 0.098743\n",
      "epoch 108, minibatch 183/183, validation loss 0.197647\n",
      "KL loss: -0.000067, CE loss: 0.164226\n",
      "epoch training accuracy: 0.942969, 0.098743\n",
      "epoch 109, minibatch 183/183, validation loss 0.197643\n",
      "KL loss: -0.000201, CE loss: 0.163602\n",
      "epoch training accuracy: 0.942951, 0.098743\n",
      "epoch 110, minibatch 183/183, validation loss 0.197287\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000099, CE loss: 0.164264\n",
      "epoch training accuracy: 0.943206, 0.098743\n",
      "epoch 111, minibatch 183/183, validation loss 0.197014\n",
      "test accuracy: 0.097879\n",
      "KL loss: -0.000178, CE loss: 0.163148\n",
      "epoch training accuracy: 0.943078, 0.098743\n",
      "epoch 112, minibatch 183/183, validation loss 0.197303\n",
      "KL loss: -0.000119, CE loss: 0.162379\n",
      "epoch training accuracy: 0.943515, 0.098743\n",
      "epoch 113, minibatch 183/183, validation loss 0.197928\n",
      "KL loss: -0.000127, CE loss: 0.161999\n",
      "epoch training accuracy: 0.943643, 0.098743\n",
      "epoch 114, minibatch 183/183, validation loss 0.197302\n",
      "KL loss: -0.000136, CE loss: 0.161636\n",
      "epoch training accuracy: 0.943698, 0.098743\n",
      "epoch 115, minibatch 183/183, validation loss 0.197406\n",
      "KL loss: -0.000157, CE loss: 0.162599\n",
      "epoch training accuracy: 0.943916, 0.098743\n",
      "epoch 116, minibatch 183/183, validation loss 0.197431\n",
      "KL loss: -0.000188, CE loss: 0.162273\n",
      "epoch training accuracy: 0.943734, 0.098743\n",
      "epoch 117, minibatch 183/183, validation loss 0.197602\n",
      "KL loss: -0.000151, CE loss: 0.162417\n",
      "epoch training accuracy: 0.943898, 0.098743\n",
      "epoch 118, minibatch 183/183, validation loss 0.197507\n",
      "KL loss: -0.000112, CE loss: 0.161726\n",
      "epoch training accuracy: 0.943789, 0.098743\n",
      "epoch 119, minibatch 183/183, validation loss 0.197902\n",
      "KL loss: -0.000108, CE loss: 0.160721\n",
      "epoch training accuracy: 0.944062, 0.098743\n",
      "epoch 120, minibatch 183/183, validation loss 0.197314\n",
      "KL loss: -0.000170, CE loss: 0.162735\n",
      "epoch training accuracy: 0.944208, 0.098743\n",
      "epoch 121, minibatch 183/183, validation loss 0.197398\n",
      "KL loss: -0.000164, CE loss: 0.159616\n",
      "epoch training accuracy: 0.944062, 0.098743\n",
      "epoch 122, minibatch 183/183, validation loss 0.197396\n",
      "KL loss: -0.000261, CE loss: 0.161237\n",
      "epoch training accuracy: 0.944408, 0.098743\n",
      "epoch 123, minibatch 183/183, validation loss 0.197303\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "'''\n",
    "Define :\n",
    "    xx_loss : Cost function value\n",
    "    xx_score : Classification accuracy rate\n",
    "'''        \n",
    "    \n",
    "print('... training')\n",
    "    \n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    \n",
    "#validation_frequency = n_train_batches\n",
    "    \n",
    "best_iter = 0\n",
    "best_train_loss = np.inf\n",
    "best_validation_loss = np.inf  \n",
    "test_loss = np.inf\n",
    "train_score = 0.\n",
    "validation_score = 0.\n",
    "test_score = 0.    \n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "kl_store=[]\n",
    "ce_store=[]\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    train1_acc=[]\n",
    "    train2_acc=[]\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        [minibatch_avg_cost, KL_loss, CE_loss, pred, lab, pred_test, aa, bb, ec_m, ec_s, dc_m, dc_s] \\\n",
    "        = train_model(minibatch_index)\n",
    "        #print(minibatch_index)\n",
    "                        \n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            \n",
    "        train1_acc.append(get_acc(pred, np.nonzero(lab)[1]))\n",
    "        train2_acc.append(get_acc(pred_test, np.nonzero(lab)[1]))\n",
    "    \n",
    "            \n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in range(n_valid_batches)] \n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "                \n",
    "            print('KL loss: %f, CE loss: %f' % (np.mean(KL_loss),np.mean(CE_loss)))\n",
    "            kl_store.append(np.mean(KL_loss))\n",
    "            ce_store.append(np.mean(CE_loss))\n",
    "            #print('encoder mu: %f, sigma: %f' % (ec_m, ec_s))\n",
    "            #print('decoder mu: %f, sigma: %f' % (dc_m, dc_s))\n",
    "            #print(ec_m)\n",
    "            #print(dc_m)\n",
    "            #print(ec_s)\n",
    "            #print(dc_s)\n",
    "            \n",
    "            print('epoch training accuracy: %f, %f' \\\n",
    "                % (np.mean(np.array(train1_acc)), np.mean(np.array(train2_acc))))\n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, validation loss %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (\n",
    "                    this_validation_loss < best_validation_loss *\n",
    "                    improvement_threshold\n",
    "                ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss   \n",
    "                best_iter = iter\n",
    "                '''\n",
    "                # get training accuracy\n",
    "                print('best training accuracy: %f' % (np.mean(np.array(train_acc))))\n",
    "                # test it on the test set\n",
    "                #test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                #test_score = np.mean(test_losses)\n",
    "\n",
    "                print(('epoch %i, minibatch %i/%i, best train accuracy: %f') % \\\n",
    "                        (epoch, minibatch_index + 1, n_train_batches, \\\n",
    "                        np.mean(np.array(train_acc))))\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                test_acc=[]\n",
    "                for minibatch_index in range(n_test_batches):\n",
    "                    [pred_test, lab_test]= test_model(minibatch_index)\n",
    "                    #print(pred)\n",
    "                    #print(np.nonzero(lab)[1])\n",
    "                    # iteration number\n",
    "                    iter = minibatch_index\n",
    "                    test_acc.append(get_acc(pred_test, np.nonzero(lab_test)[1]))\n",
    "    \n",
    "                print('test accuracy: %f' % (np.mean(test_acc)))\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "    #print('~~~~~~~~~~~~~~~test z:~~~~~~~~~~~~~~~~')\n",
    "    #print(test_EC_z[-1])\n",
    "    #print('===============train z================')     \n",
    "    #print(EC_z[-2])\n",
    "    #print(EC_z[-1])\n",
    "    #print(DC_z[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
