{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from six.moves import cPickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import nnet as nn\n",
    "import criteria as er\n",
    "import util\n",
    "import VAE\n",
    "import SVAE\n",
    "import update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_acc(pred, true):\n",
    "    ll = pred - true\n",
    "    ll = np.array(ll)\n",
    "    acc = 1 - (np.nonzero(ll)[0].shape[0])/float(ll.shape[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set\n",
      "Loading valida\n",
      "number of minibatch at one epoch: train  1191, validation 186\n"
     ]
    }
   ],
   "source": [
    "'''Load Data'''\n",
    "train_file = 'timit_train_subset_1ok.npy'\n",
    "valid_file = 'timit_valid_1ok.npy'\n",
    "#test_file = 'test_1ok.npy'\n",
    "    \n",
    "train=np.load(train_file)\n",
    "valid=np.load(valid_file)\n",
    "#test=np.load(test_file)\n",
    "\n",
    "    \n",
    "\n",
    "print('Loading training set')    \n",
    "train_feat, train_label = util.shared_dataset_timit(train)\n",
    "print('Loading valida')\n",
    "valid_feat, valid_label = util.shared_dataset_timit(valid)\n",
    "#test_feat, test_label = util.shared_dataset(test) \n",
    "    \n",
    "  \n",
    "'''Coefficient Initial'''        \n",
    "batch_size = 300\n",
    "n_epochs = 5\n",
    "learning_rate = 0.1\n",
    "    \n",
    "n_train_batches = train_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "#n_test_batches = test_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "print('number of minibatch at one epoch: train  %i, validation %i' %\n",
    "    (n_train_batches, n_valid_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357376\n",
      "357376\n",
      "440\n",
      "1984\n"
     ]
    }
   ],
   "source": [
    "z_dim = 5 #dimension of latent variable \n",
    "x_dim = train_feat.get_value(borrow=True).shape[1]\n",
    "y_dim = train_label.get_value(borrow=True).shape[1]\n",
    "activation = None\n",
    "    \n",
    "print(train_feat.get_value(borrow=True).shape[0])\n",
    "print(train_label.get_value(borrow=True).shape[0])\n",
    "print(train_feat.get_value(borrow=True).shape[1])\n",
    "print(train_label.get_value(borrow=True).shape[1])\n",
    "\n",
    "phi_1_struct=nn.NN_struct()\n",
    "phi_1_struct.layer_dim = [x_dim+y_dim, 500, z_dim]\n",
    "phi_1_struct.activation = [None, None]\n",
    "    \n",
    "theta_1_struct=nn.NN_struct()\n",
    "theta_1_struct.layer_dim = [x_dim, 500, z_dim]\n",
    "theta_1_struct.activation = [None, None]\n",
    "\n",
    "theta_2_struct=nn.NN_struct()\n",
    "theta_2_struct.layer_dim = [z_dim+x_dim, 500, y_dim]\n",
    "theta_2_struct.activation = [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()  # index to a [mini]batch\n",
    "x = T.matrix('x')  # the data is presented as rasterized images\n",
    "y = T.matrix('y')  # the labels are presented as signal vector   \n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "with open('timit_model_step_ce_1l_500h_z5.save', 'rb') as f:\n",
    "    model = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2424, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.phi_mu.HL_1.W.get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_kl = SVAE.Supervised_VAE_v3_KL_1(\n",
    "    rng=rng,\n",
    "    input_x = x,\n",
    "    label_y = y,\n",
    "    batch_size = batch_size,\n",
    "    phi_1_struct = phi_1_struct,\n",
    "    theta_1_struct = theta_1_struct,\n",
    "    theta_2_struct = theta_2_struct,\n",
    "    in_dim = x_dim,\n",
    "    out_dim = y_dim,\n",
    "    model = model\n",
    "    )\n",
    "\n",
    "#cost = (classifier_kl.cost)\n",
    "        \n",
    "#gparams = [T.grad(cost, param) for param in classifier_kl.params]\n",
    "                   \n",
    "#updates = [\n",
    "#    (param, param - learning_rate * gparam)\n",
    "#    for param, gparam in zip(classifier_kl.params, gparams)\n",
    "#]\n",
    "\n",
    "updates = update.adam(loss=classifier_kl.cost, all_params=classifier_kl.params, learning_rate=0.005)\n",
    "\n",
    "#classifier_kl.phi_mu = model.phi_mu\n",
    "#classifier_kl.phi_sigma = model.phi_sigma\n",
    "#classifier_kl.theta_2 = model.theta_2\n",
    "#classifier_kl.predict = model.predict\n",
    "#print(classifier_kl.phi_sigma.OL.W.get_value())\n",
    "#print(model.phi_sigma.OL.W.get_value())\n",
    "#print(classifier_kl.theta_2.OL.W.get_value())\n",
    "#print(model.theta_2.OL.W.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... prepare training model\n",
      "... prepare validate model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('... prepare test model')\\ntest_model = theano.function(\\n    inputs=[index],\\n    outputs=[classifier_kl.predictor_test, classifier_kl.label_y],\\n    givens={\\n        x: test_feat[index * batch_size : (index + 1) * batch_size, :],\\n        y: test_label[index * batch_size : (index + 1) * batch_size, :]\\n    }        \\n)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('... prepare training model')\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier_kl.cost, classifier_kl.predictor, classifier_kl.label_y, classifier_kl.predictor_test,\n",
    "             classifier_kl.EC_mu, classifier_kl.EC_log_sigma, classifier_kl.DC_mu, classifier_kl.DC_log_sigma,\n",
    "             classifier_kl.KL, classifier_kl.CE],\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: train_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }       \n",
    ")   \n",
    "    \n",
    "print('... prepare validate model')\n",
    "validate_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier_kl.cost,\n",
    "    givens={\n",
    "        x: valid_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: valid_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")      \n",
    "\n",
    "'''\n",
    "print('... prepare test model')\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier_kl.predictor_test, classifier_kl.label_y],\n",
    "    givens={\n",
    "        x: test_feat[index * batch_size : (index + 1) * batch_size, :],\n",
    "        y: test_label[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "minibatch 0/1191\n",
      "minibatch 100/1191\n",
      "minibatch 200/1191\n",
      "minibatch 300/1191\n",
      "minibatch 400/1191\n",
      "minibatch 500/1191\n",
      "minibatch 600/1191\n",
      "minibatch 700/1191\n",
      "minibatch 800/1191\n",
      "minibatch 900/1191\n",
      "minibatch 1000/1191\n",
      "minibatch 1100/1191\n",
      "encoder mean: array([[-1.91571891, -3.02916837,  0.62025774, -1.65124297,  4.14824581],\n",
      "       [ 0.24282704, -1.47241306, -1.71033776, -0.89856398,  1.54645383],\n",
      "       [-1.87905395, -1.31926012, -0.52835846,  0.38916755,  1.48157704],\n",
      "       ..., \n",
      "       [-1.56605601, -3.15024328,  0.97980988, -1.23374093,  4.13326168],\n",
      "       [-1.59111643,  1.34922528, -0.22490703, -0.90497148,  1.22909892],\n",
      "       [-0.08070608,  0.42106038,  0.71342754, -2.04163551, -0.67803818]], dtype=float32) \n",
      " encoder log variance: array([[ -8.65079975,  -9.54172134,  -8.74419117,  -7.53085947,\n",
      "         -9.73812485],\n",
      "       [ -9.09579945,  -9.51011848,  -9.08875465,  -9.44527626,\n",
      "         -9.06989193],\n",
      "       [ -6.99052334,  -8.57307434,  -8.64884949,  -9.66434765,\n",
      "         -9.30735207],\n",
      "       ..., \n",
      "       [ -8.5817461 ,  -9.65552616,  -8.84937191,  -7.69509554,\n",
      "         -9.81938553],\n",
      "       [ -8.39962482,  -9.16248512,  -9.31552124, -10.07376766,\n",
      "         -9.40462494],\n",
      "       [ -9.29938507,  -8.90118599,  -8.53799057,  -9.79922771,\n",
      "         -9.08388042]], dtype=float32) \n",
      " decoder mean: array([[-1.69830513, -2.60773516,  1.5637269 , -2.29952216,  1.37899828],\n",
      "       [ 0.30445546, -1.87692451, -0.1785856 , -0.25358132, -0.02405533],\n",
      "       [-1.29896772, -0.50345641, -0.01215376,  0.31476676,  0.81515676],\n",
      "       ..., \n",
      "       [-1.13862038, -2.49146819,  1.61660492, -2.01874471,  1.17497945],\n",
      "       [-1.06076932, -0.29093209,  1.22798681, -0.65338051, -0.53983867],\n",
      "       [-1.15573394, -0.68683946,  1.21594238, -1.035061  , -0.2942602 ]], dtype=float32) \n",
      " decoder log variance: array([[ 0.69184935,  1.34133172,  2.45584226,  0.71908724,  2.93272424],\n",
      "       [ 0.19295254,  0.10820813,  0.95030057,  0.13975278,  1.03599823],\n",
      "       [ 0.27059484,  0.4193792 ,  0.74178118,  0.07923479,  1.09438753],\n",
      "       ..., \n",
      "       [ 0.74083805,  1.50992095,  2.4967618 ,  0.83729541,  2.81018567],\n",
      "       [ 0.4725813 ,  0.43986607,  0.53454506,  0.45096144,  0.66440535],\n",
      "       [ 0.3275328 ,  0.1911442 ,  0.70918459,  0.28115121,  0.75404131]], dtype=float32) \n",
      "\n",
      "kl loss: -23.893515, ce loss: 3.106063 \n",
      "\n",
      "epoch training accuracy ec: 0.988228, dc: 0.222004, training loss: 26.999578\n",
      "epoch 1, minibatch 1191/1191, validation loss 27.073694\n",
      "minibatch 9/1191\n",
      "minibatch 109/1191\n",
      "minibatch 209/1191\n",
      "minibatch 309/1191\n",
      "minibatch 409/1191\n",
      "minibatch 509/1191\n",
      "minibatch 609/1191\n",
      "minibatch 709/1191\n",
      "minibatch 809/1191\n",
      "minibatch 909/1191\n",
      "minibatch 1009/1191\n",
      "minibatch 1109/1191\n",
      "encoder mean: array([[-1.91571891, -3.02916837,  0.62025774, -1.65124297,  4.14824581],\n",
      "       [ 0.24282704, -1.47241306, -1.71033776, -0.89856398,  1.54645383],\n",
      "       [-1.87905395, -1.31926012, -0.52835846,  0.38916755,  1.48157704],\n",
      "       ..., \n",
      "       [-1.56605601, -3.15024328,  0.97980988, -1.23374093,  4.13326168],\n",
      "       [-1.59111643,  1.34922528, -0.22490703, -0.90497148,  1.22909892],\n",
      "       [-0.08070608,  0.42106038,  0.71342754, -2.04163551, -0.67803818]], dtype=float32) \n",
      " encoder log variance: array([[ -8.65079975,  -9.54172134,  -8.74419117,  -7.53085947,\n",
      "         -9.73812485],\n",
      "       [ -9.09579945,  -9.51011848,  -9.08875465,  -9.44527626,\n",
      "         -9.06989193],\n",
      "       [ -6.99052334,  -8.57307434,  -8.64884949,  -9.66434765,\n",
      "         -9.30735207],\n",
      "       ..., \n",
      "       [ -8.5817461 ,  -9.65552616,  -8.84937191,  -7.69509554,\n",
      "         -9.81938553],\n",
      "       [ -8.39962482,  -9.16248512,  -9.31552124, -10.07376766,\n",
      "         -9.40462494],\n",
      "       [ -9.29938507,  -8.90118599,  -8.53799057,  -9.79922771,\n",
      "         -9.08388042]], dtype=float32) \n",
      " decoder mean: array([[-1.57557809, -2.80278015,  1.55156314, -2.43740606,  1.14989662],\n",
      "       [ 0.39765817, -1.62786925, -0.33955508, -0.2341205 , -0.22632356],\n",
      "       [-1.37913549, -0.5203675 ,  0.23727956,  0.25914535,  0.63844609],\n",
      "       ..., \n",
      "       [-1.26793957, -2.59659815,  1.60708821, -2.1371274 ,  1.01601374],\n",
      "       [-1.03014243, -0.10098329,  1.10137451, -0.62309617, -0.94778097],\n",
      "       [-1.17917705, -0.40258265,  0.92847228, -0.89059997, -0.22221792]], dtype=float32) \n",
      " decoder log variance: array([[ 0.68919015,  1.03359175,  2.18965006,  0.63161868,  2.88600612],\n",
      "       [-0.02101658,  0.05450412,  0.85594136, -0.06797842,  1.00359869],\n",
      "       [ 0.42120403,  0.55726224,  0.27578053, -0.01908425,  0.9046306 ],\n",
      "       ..., \n",
      "       [ 0.75532508,  1.26147759,  2.32888055,  0.68139595,  2.79888964],\n",
      "       [ 0.47799692,  0.81292003,  0.75819635,  0.58188647,  0.88167268],\n",
      "       [ 0.54109395,  0.12582543,  0.37505016, -0.03554504,  0.67367697]], dtype=float32) \n",
      "\n",
      "kl loss: -23.808079, ce loss: 2.839843 \n",
      "\n",
      "epoch training accuracy ec: 0.988223, dc: 0.304123, training loss: 26.647923\n",
      "epoch 2, minibatch 1191/1191, validation loss 26.882570\n",
      "minibatch 18/1191\n",
      "minibatch 118/1191\n",
      "minibatch 218/1191\n",
      "minibatch 318/1191\n",
      "minibatch 418/1191\n",
      "minibatch 518/1191\n",
      "minibatch 618/1191\n",
      "minibatch 718/1191\n",
      "minibatch 818/1191\n",
      "minibatch 918/1191\n",
      "minibatch 1018/1191\n",
      "minibatch 1118/1191\n",
      "encoder mean: array([[-1.91571891, -3.02916837,  0.62025774, -1.65124297,  4.14824581],\n",
      "       [ 0.24282704, -1.47241306, -1.71033776, -0.89856398,  1.54645383],\n",
      "       [-1.87905395, -1.31926012, -0.52835846,  0.38916755,  1.48157704],\n",
      "       ..., \n",
      "       [-1.56605601, -3.15024328,  0.97980988, -1.23374093,  4.13326168],\n",
      "       [-1.59111643,  1.34922528, -0.22490703, -0.90497148,  1.22909892],\n",
      "       [-0.08070608,  0.42106038,  0.71342754, -2.04163551, -0.67803818]], dtype=float32) \n",
      " encoder log variance: array([[ -8.65079975,  -9.54172134,  -8.74419117,  -7.53085947,\n",
      "         -9.73812485],\n",
      "       [ -9.09579945,  -9.51011848,  -9.08875465,  -9.44527626,\n",
      "         -9.06989193],\n",
      "       [ -6.99052334,  -8.57307434,  -8.64884949,  -9.66434765,\n",
      "         -9.30735207],\n",
      "       ..., \n",
      "       [ -8.5817461 ,  -9.65552616,  -8.84937191,  -7.69509554,\n",
      "         -9.81938553],\n",
      "       [ -8.39962482,  -9.16248512,  -9.31552124, -10.07376766,\n",
      "         -9.40462494],\n",
      "       [ -9.29938507,  -8.90118599,  -8.53799057,  -9.79922771,\n",
      "         -9.08388042]], dtype=float32) \n",
      " decoder mean: array([[-1.73549628, -2.8223958 ,  1.96323824, -2.62365532,  0.67094976],\n",
      "       [ 0.2460601 , -1.66064847,  0.21701901, -0.13826016, -0.63049352],\n",
      "       [-1.32098162, -0.22727101, -0.29766214,  0.54787719,  0.92112321],\n",
      "       ..., \n",
      "       [-1.1674726 , -2.57285929,  1.78750026, -2.1457386 ,  0.50464964],\n",
      "       [-1.0979352 ,  0.16208868,  1.14499617, -0.56597495, -0.50128782],\n",
      "       [-1.07054424, -0.53194177,  0.99819738, -0.88682169, -0.29958922]], dtype=float32) \n",
      " decoder log variance: array([[ 0.57686001,  1.05064809,  2.03616643,  0.39988747,  2.73789048],\n",
      "       [ 0.27453116,  0.44567022,  1.07908654, -0.05748913,  1.24608052],\n",
      "       [ 0.37752861,  0.7982626 ,  0.47285223,  0.12735581,  0.60380995],\n",
      "       ..., \n",
      "       [ 0.71388292,  1.2773031 ,  2.21361661,  0.61398393,  2.80656886],\n",
      "       [ 0.53014231,  0.96219611,  0.71598977,  0.61149627,  0.85601526],\n",
      "       [ 0.39195597,  0.34159383,  0.54888994, -0.02957729,  0.65097213]], dtype=float32) \n",
      "\n",
      "kl loss: -23.703047, ce loss: 2.664314 \n",
      "\n",
      "epoch training accuracy ec: 0.988223, dc: 0.337084, training loss: 26.367361\n",
      "epoch 3, minibatch 1191/1191, validation loss 26.849468\n",
      "minibatch 27/1191\n",
      "minibatch 127/1191\n",
      "minibatch 227/1191\n",
      "minibatch 327/1191\n",
      "minibatch 427/1191\n",
      "minibatch 527/1191\n",
      "minibatch 627/1191\n",
      "minibatch 727/1191\n",
      "minibatch 827/1191\n",
      "minibatch 927/1191\n",
      "minibatch 1027/1191\n",
      "minibatch 1127/1191\n",
      "encoder mean: array([[-1.91571891, -3.02916837,  0.62025774, -1.65124297,  4.14824581],\n",
      "       [ 0.24282704, -1.47241306, -1.71033776, -0.89856398,  1.54645383],\n",
      "       [-1.87905395, -1.31926012, -0.52835846,  0.38916755,  1.48157704],\n",
      "       ..., \n",
      "       [-1.56605601, -3.15024328,  0.97980988, -1.23374093,  4.13326168],\n",
      "       [-1.59111643,  1.34922528, -0.22490703, -0.90497148,  1.22909892],\n",
      "       [-0.08070608,  0.42106038,  0.71342754, -2.04163551, -0.67803818]], dtype=float32) \n",
      " encoder log variance: array([[ -8.65079975,  -9.54172134,  -8.74419117,  -7.53085947,\n",
      "         -9.73812485],\n",
      "       [ -9.09579945,  -9.51011848,  -9.08875465,  -9.44527626,\n",
      "         -9.06989193],\n",
      "       [ -6.99052334,  -8.57307434,  -8.64884949,  -9.66434765,\n",
      "         -9.30735207],\n",
      "       ..., \n",
      "       [ -8.5817461 ,  -9.65552616,  -8.84937191,  -7.69509554,\n",
      "         -9.81938553],\n",
      "       [ -8.39962482,  -9.16248512,  -9.31552124, -10.07376766,\n",
      "         -9.40462494],\n",
      "       [ -9.29938507,  -8.90118599,  -8.53799057,  -9.79922771,\n",
      "         -9.08388042]], dtype=float32) \n",
      " decoder mean: array([[-1.72313106, -2.74174666,  1.59398544, -2.47057557,  1.06308758],\n",
      "       [ 0.51916289, -1.02999365, -0.08918552, -0.35824981, -0.86184549],\n",
      "       [-1.14819372, -0.47700197, -0.06804225,  0.65091616,  0.89121687],\n",
      "       ..., \n",
      "       [-1.16683888, -2.57674646,  1.60968244, -2.08224082,  0.82255542],\n",
      "       [-0.87148511, -0.05076934,  1.32905495, -0.61709321, -0.49773249],\n",
      "       [-1.32086086, -0.58707005,  1.48701191, -0.97503513, -0.99790674]], dtype=float32) \n",
      " decoder log variance: array([[ 0.8516438 ,  1.22728765,  2.22447896,  0.6842553 ,  3.08353043],\n",
      "       [-0.12501717,  0.11314377,  0.67410702, -0.34952176,  0.89140755],\n",
      "       [ 0.1942755 ,  0.68869156,  0.42351478, -0.07737695,  0.66628557],\n",
      "       ..., \n",
      "       [ 0.7990253 ,  1.28869474,  2.21951365,  0.79145342,  3.04144478],\n",
      "       [ 0.51733303,  0.86531538,  0.65830362,  0.36849079,  0.73585445],\n",
      "       [ 0.24894755, -0.09571256,  0.30361372, -0.12339018,  0.6622681 ]], dtype=float32) \n",
      "\n",
      "kl loss: -23.770313, ce loss: 2.460955 \n",
      "\n",
      "epoch training accuracy ec: 0.988228, dc: 0.361338, training loss: 26.231268\n",
      "epoch 4, minibatch 1191/1191, validation loss 26.894657\n",
      "minibatch 36/1191\n",
      "minibatch 136/1191\n",
      "minibatch 236/1191\n",
      "minibatch 336/1191\n",
      "minibatch 436/1191\n",
      "minibatch 536/1191\n",
      "minibatch 636/1191\n",
      "minibatch 736/1191\n",
      "minibatch 836/1191\n",
      "minibatch 936/1191\n",
      "minibatch 1036/1191\n",
      "minibatch 1136/1191\n",
      "encoder mean: array([[-1.91571891, -3.02916837,  0.62025774, -1.65124297,  4.14824581],\n",
      "       [ 0.24282704, -1.47241306, -1.71033776, -0.89856398,  1.54645383],\n",
      "       [-1.87905395, -1.31926012, -0.52835846,  0.38916755,  1.48157704],\n",
      "       ..., \n",
      "       [-1.56605601, -3.15024328,  0.97980988, -1.23374093,  4.13326168],\n",
      "       [-1.59111643,  1.34922528, -0.22490703, -0.90497148,  1.22909892],\n",
      "       [-0.08070608,  0.42106038,  0.71342754, -2.04163551, -0.67803818]], dtype=float32) \n",
      " encoder log variance: array([[ -8.65079975,  -9.54172134,  -8.74419117,  -7.53085947,\n",
      "         -9.73812485],\n",
      "       [ -9.09579945,  -9.51011848,  -9.08875465,  -9.44527626,\n",
      "         -9.06989193],\n",
      "       [ -6.99052334,  -8.57307434,  -8.64884949,  -9.66434765,\n",
      "         -9.30735207],\n",
      "       ..., \n",
      "       [ -8.5817461 ,  -9.65552616,  -8.84937191,  -7.69509554,\n",
      "         -9.81938553],\n",
      "       [ -8.39962482,  -9.16248512,  -9.31552124, -10.07376766,\n",
      "         -9.40462494],\n",
      "       [ -9.29938507,  -8.90118599,  -8.53799057,  -9.79922771,\n",
      "         -9.08388042]], dtype=float32) \n",
      " decoder mean: array([[-1.64033353, -2.67518139,  1.3670727 , -2.49761534,  1.07082415],\n",
      "       [ 0.60834503, -1.08755624, -0.31378809, -0.13277173, -0.47880819],\n",
      "       [-1.32732737, -0.95049739,  0.17247331,  0.56636173,  1.18001115],\n",
      "       ..., \n",
      "       [-1.4444803 , -2.53556085,  1.38018143, -2.18816781,  1.0668509 ],\n",
      "       [-1.20871699,  0.39653978,  1.05448234, -0.32223728, -0.71424544],\n",
      "       [-0.57970643, -0.37042242,  0.9526087 , -1.08641815, -0.44996148]], dtype=float32) \n",
      " decoder log variance: array([[ 0.61689597,  1.11512423,  2.01902676,  0.62155867,  2.80740428],\n",
      "       [-0.12140545,  0.17582659,  1.01535344, -0.12261788,  1.04052162],\n",
      "       [ 0.20726754,  0.13102236,  0.03083907, -0.26253152,  0.36829561],\n",
      "       ..., \n",
      "       [ 0.87955958,  1.47745752,  2.37740374,  0.84901702,  3.06221938],\n",
      "       [ 0.56120229,  0.85205036,  0.46545866,  0.2977514 ,  0.92970538],\n",
      "       [ 0.4201014 ,  0.10659998,  0.40783253, -0.17705095,  0.69410574]], dtype=float32) \n",
      "\n",
      "kl loss: -23.750561, ce loss: 2.364723 \n",
      "\n",
      "epoch training accuracy ec: 0.988240, dc: 0.374467, training loss: 26.115284\n",
      "epoch 5, minibatch 1191/1191, validation loss 27.001328\n",
      "minibatch 45/1191\n",
      "minibatch 145/1191\n",
      "minibatch 245/1191\n",
      "minibatch 345/1191\n",
      "minibatch 445/1191\n",
      "minibatch 545/1191\n",
      "minibatch 645/1191\n",
      "minibatch 745/1191\n",
      "minibatch 845/1191\n",
      "minibatch 945/1191\n",
      "minibatch 1045/1191\n",
      "minibatch 1145/1191\n",
      "encoder mean: array([[-1.91571891, -3.02916837,  0.62025774, -1.65124297,  4.14824581],\n",
      "       [ 0.24282704, -1.47241306, -1.71033776, -0.89856398,  1.54645383],\n",
      "       [-1.87905395, -1.31926012, -0.52835846,  0.38916755,  1.48157704],\n",
      "       ..., \n",
      "       [-1.56605601, -3.15024328,  0.97980988, -1.23374093,  4.13326168],\n",
      "       [-1.59111643,  1.34922528, -0.22490703, -0.90497148,  1.22909892],\n",
      "       [-0.08070608,  0.42106038,  0.71342754, -2.04163551, -0.67803818]], dtype=float32) \n",
      " encoder log variance: array([[ -8.65079975,  -9.54172134,  -8.74419117,  -7.53085947,\n",
      "         -9.73812485],\n",
      "       [ -9.09579945,  -9.51011848,  -9.08875465,  -9.44527626,\n",
      "         -9.06989193],\n",
      "       [ -6.99052334,  -8.57307434,  -8.64884949,  -9.66434765,\n",
      "         -9.30735207],\n",
      "       ..., \n",
      "       [ -8.5817461 ,  -9.65552616,  -8.84937191,  -7.69509554,\n",
      "         -9.81938553],\n",
      "       [ -8.39962482,  -9.16248512,  -9.31552124, -10.07376766,\n",
      "         -9.40462494],\n",
      "       [ -9.29938507,  -8.90118599,  -8.53799057,  -9.79922771,\n",
      "         -9.08388042]], dtype=float32) \n",
      " decoder mean: array([[ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       ..., \n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan]], dtype=float32) \n",
      " decoder log variance: array([[ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       ..., \n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan]], dtype=float32) \n",
      "\n",
      "kl loss: nan, ce loss: nan \n",
      "\n",
      "epoch training accuracy ec: 0.988240, dc: 0.052208, training loss: nan\n",
      "epoch 6, minibatch 1191/1191, validation loss nan\n",
      "minibatch 54/1191\n",
      "minibatch 154/1191\n",
      "minibatch 254/1191\n",
      "minibatch 354/1191\n",
      "minibatch 454/1191\n",
      "minibatch 554/1191\n",
      "minibatch 654/1191\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-41398de0252e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mminibatch_avg_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mec_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mec_log_sigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdc_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdc_log_sigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m]\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;31m#print(minibatch_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out)\u001b[0m\n\u001b[0;32m   5434\u001b[0m         \u001b[1;31m# gives a numpy float object but we need to return a 0d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5435\u001b[0m         \u001b[1;31m# ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5436\u001b[1;33m         \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "'''\n",
    "Define :\n",
    "    xx_loss : Cost function value\n",
    "    xx_score : Classification accuracy rate\n",
    "'''        \n",
    "    \n",
    "print('... training')\n",
    "    \n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    \n",
    "#validation_frequency = n_train_batches\n",
    "    \n",
    "best_iter = 0\n",
    "best_train_loss = np.inf\n",
    "best_validation_loss = np.inf  \n",
    "test_loss = np.inf\n",
    "train_score = 0.\n",
    "validation_score = 0.\n",
    "test_score = 0.    \n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    train_acc1=[]\n",
    "    train_acc2=[]\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        [minibatch_avg_cost, pred1, lab, pred2, ec_mu, ec_log_sigma, dc_mu, dc_log_sigma, kl, ce] \\\n",
    "        = train_model(minibatch_index)\n",
    "        #print(minibatch_index)\n",
    "                        \n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        if iter % 100 == 0:\n",
    "            print('minibatch %i/%i' % (minibatch_index, n_train_batches))\n",
    "        #print(pred1)\n",
    "        #print(classifier_kl.phi_sigma.OL.W.get_value())\n",
    "        train_acc1.append(get_acc(pred1, np.nonzero(lab)[1]))\n",
    "        train_acc2.append(get_acc(pred2, np.nonzero(lab)[1]))\n",
    "            \n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in range(n_valid_batches)] \n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "                \n",
    "            #print('CE loss: %f' % (np.mean(KL_loss)))\n",
    "            #print('encoder mu: %f, sigma: %f' % (ec_m, ec_s))\n",
    "            #print('decoder mu: %f, sigma: %f' % (dc_m, dc_s))\n",
    "            #print(ec_m)\n",
    "            #print(dc_m)\n",
    "            #print(ec_s)\n",
    "            #print(dc_s)\n",
    "            \n",
    "            #print('epoch training accuracy ec: %f, dc: %f, training loss: %f \\n trainig kl: %f, ce: %f' \\\n",
    "            #    % (np.mean(np.array(train_acc1)), np.mean(np.array(train_acc2)),np.mean(minibatch_avg_cost), \\\n",
    "            #      np.mean(kl), np.mean(ce)))\n",
    "            print('encoder mean: %r \\n encoder log variance: %r \\n decoder mean: %r \\n decoder log variance: %r \\n' \\\n",
    "                  % (ec_mu, ec_log_sigma, dc_mu, dc_log_sigma))\n",
    "            print('kl loss: %f, ce loss: %f \\n' \\\n",
    "                % (kl.mean() ,ce.mean()))\n",
    "            print('epoch training accuracy ec: %f, dc: %f, training loss: %f' \\\n",
    "                % (np.mean(np.array(train_acc1)), np.mean(np.array(train_acc2)),np.mean(minibatch_avg_cost)))\n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, validation loss %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (\n",
    "                    this_validation_loss < best_validation_loss *\n",
    "                    improvement_threshold\n",
    "                ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss   \n",
    "                best_iter = iter\n",
    "                \n",
    "                '''\n",
    "                # get training accuracy\n",
    "                print('best training accuracy: %f' % (np.mean(np.array(train_acc))))\n",
    "                # test it on the test set\n",
    "                #test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                #test_score = np.mean(test_losses)\n",
    "\n",
    "                print(('epoch %i, minibatch %i/%i, best train accuracy: %f') % \\\n",
    "                        (epoch, minibatch_index + 1, n_train_batches, \\\n",
    "                        np.mean(np.array(train_acc))))\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                \n",
    "                '''\n",
    "                test_acc=[]\n",
    "                for minibatch_index in range(n_test_batches):\n",
    "                    [pred_test, lab_test]= test_model(minibatch_index)\n",
    "                    #print(pred)\n",
    "                    #print(np.nonzero(lab)[1])\n",
    "                    # iteration number\n",
    "                    iter = minibatch_index\n",
    "                    test_acc.append(get_acc(pred_test, np.nonzero(lab_test)[1]))\n",
    "    \n",
    "                print('test accuracy: %f' % (np.mean(test_acc)))\n",
    "                '''\n",
    "                \n",
    "\n",
    "        #if patience <= iter:\n",
    "        #    done_looping = True\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
