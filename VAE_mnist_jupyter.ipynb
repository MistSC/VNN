{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from six.moves import cPickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import nnet as nn\n",
    "import criteria as er\n",
    "import util\n",
    "import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Load Data'''\n",
    "train_file = 'train_1ok.npy'\n",
    "valid_file = 'valid_1ok.npy'\n",
    "test_file = 'test_1ok.npy'\n",
    "    \n",
    "train=np.load(train_file)\n",
    "valid=np.load(valid_file)\n",
    "test=np.load(test_file)\n",
    "    \n",
    "#train_list=np.load('train.npy')[1]\n",
    "    \n",
    "\n",
    "train_feat, train_label = util.shared_dataset(train)\n",
    "valid_feat, valid_label = util.shared_dataset(valid)\n",
    "test_feat, test_label = util.shared_dataset(test)\n",
    "    \n",
    "  \n",
    "'''Coefficient Initial'''        \n",
    "batch_size = 400\n",
    "n_epochs = 100\n",
    "learning_rate = 0.05\n",
    "    \n",
    "n_train_batches = train_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "n_test_batches = test_feat.get_value(borrow=True).shape[0] // batch_size\n",
    "print('number of minibatch at one epoch: train  %i, validation %i, test %i' %\n",
    "    (n_train_batches, n_valid_batches, n_test_batches))\n",
    "    \n",
    "z_dim = 5 #dimension of latent variable \n",
    "x_dim = train_feat.get_value(borrow=True).shape[1]\n",
    "y_dim = train_label.get_value(borrow=True).shape[1]\n",
    "activation = None\n",
    "    \n",
    "print(train_feat.get_value(borrow=True).shape[0])\n",
    "print(train_label.get_value(borrow=True).shape[0])\n",
    "print(train_feat.get_value(borrow=True).shape[1])\n",
    "print(train_label.get_value(borrow=True).shape[1])\n",
    "\n",
    "phi_1_struct=nn.NN_struct()\n",
    "phi_1_struct.layer_dim = [x_dim, z_dim]\n",
    "phi_1_struct.activation = [activation]\n",
    "    \n",
    "theta_1_struct=nn.NN_struct()\n",
    "theta_1_struct.layer_dim = [z_dim, x_dim]\n",
    "theta_1_struct.activation = [activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# BUILD ACTUAL MODEL #\n",
    "######################\n",
    "print('... building the model')\n",
    "    \n",
    "    \n",
    "# allocate symbolic variables for the data\n",
    "#index_source = T.lscalar()  # index to a [mini]batch\n",
    "#index_target = T.lscalar()  # index to a [mini]batch\n",
    "index = T.lscalar()  # index to a [mini]batch\n",
    "x = T.matrix('x')  # the data is presented as rasterized images\n",
    "y = T.matrix('y')  # the labels are presented as signal vector     \n",
    "    \n",
    "rng = np.random.RandomState(1234)\n",
    "        \n",
    "# construct the DAVAE class\n",
    "   \n",
    "classifier = VAE.Unsupervised_VAE(\n",
    "    rng=rng,\n",
    "    input_x = x,\n",
    "    label_y = x,\n",
    "    batch_size = batch_size,\n",
    "    phi_1_struct = phi_1_struct,\n",
    "    theta_1_struct = theta_1_struct,\n",
    "    in_dim = x_dim,\n",
    "    out_dim = x_dim\n",
    "    )\n",
    "    \n",
    "    \n",
    "cost = (classifier.cost)\n",
    "        \n",
    "gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "                   \n",
    "updates = [\n",
    "    (param, param - learning_rate * gparam)\n",
    "    for param, gparam in zip(classifier.params, gparams)\n",
    "]\n",
    "    \n",
    "print('... prepare training model')\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=[classifier.cost, classifier.predictor, classifier.label_y],\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_feat[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }       \n",
    ")   \n",
    "    \n",
    "    \n",
    "print('... prepare validate model')\n",
    "validate_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier.cost,\n",
    "    givens={\n",
    "        x: valid_feat[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")                \n",
    "    \n",
    "    \n",
    "print('... prepare test model')\n",
    "test_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=classifier.predictor,\n",
    "    givens={\n",
    "        x: test_feat[index * batch_size : (index + 1) * batch_size, :]\n",
    "    }        \n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "'''\n",
    "Define :\n",
    "    xx_loss : Cost function value\n",
    "    xx_score : Classification accuracy rate\n",
    "'''        \n",
    "    \n",
    "print('... training')\n",
    "    \n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "    \n",
    "#validation_frequency = n_train_batches\n",
    "    \n",
    "best_iter = 0\n",
    "best_train_loss = np.inf\n",
    "best_validation_loss = np.inf  \n",
    "test_loss = np.inf\n",
    "train_score = 0.\n",
    "validation_score = 0.\n",
    "test_score = 0.    \n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "        \n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        [minibatch_avg_cost, pred, lab] = train_model(minibatch_index)\n",
    "        \n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        \n",
    "        if iter%40 == 0:\n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, training loss %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index,\n",
    "                    n_train_batches,\n",
    "                    minibatch_avg_cost\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        #train_acc.append(get_acc(pred,np.nonzero(lab)[1]))\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in range(n_valid_batches)] \n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "                \n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, validation loss %f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (\n",
    "                    this_validation_loss < best_validation_loss *\n",
    "                    improvement_threshold\n",
    "                ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                \n",
    "                #[test_pred, test_lab] = [test_model(i) for i in range(n_test_batches)] \n",
    "                #this_test_loss = np.mean(test_losses)\n",
    "                #test_acc.append(get_acc(test_pred,np.nonzero(lab)[1]))\n",
    "                    \n",
    "                    \n",
    "                best_validation_loss = this_validation_loss   \n",
    "                best_iter = iter\n",
    "          \n",
    "        \n",
    "                \n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "    '''\n",
    "    # compute loss on validation set\n",
    "        validation_losses = [validate_model(i) for i in range(n_valid_batches)] \n",
    "        this_validation_loss = np.mean(validation_losses)\n",
    "        print('epoch %i, minibatch %i/%i, validation loss %f ' %\n",
    "            (\n",
    "                epoch,\n",
    "                minibatch_index + 1,\n",
    "                n_train_batches,\n",
    "                this_validation_loss\n",
    "            )\n",
    "        )\n",
    "    '''        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred=test_model(0)\n",
    "[minibatch_avg_cost, pred, lab]=train_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaa = test_pred[12]*255//1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaa[aaa<10] = 0\n",
    "aaa=aaa.reshape(28,28)\n",
    "#print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbb=np.array(test[0][12])\n",
    "bbb=bbb.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.plot(aaa)\n",
    "plt.figure(1)\n",
    "plt.imshow(aaa*255, cmap='gray')\n",
    "plt.show()\n",
    "plt.figure(2)\n",
    "plt.imshow(bbb*255, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('model.save', 'wb') as f:\n",
    "    cPickle.dump(classifier, f, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
